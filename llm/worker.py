import json
from llama_cpp import Llama
from core.schemas import Command
from llm.prompts import SYSTEM_PROMPT # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ

class Brain:
    def __init__(self, model_path):
        self.model_path = model_path
        self.llm = None
    
    def load(self):
        print(f"ğŸ§  Loading Model from: {self.model_path}...")
        try:
            # n_ctx=4096 Ù„ÙŠØ¹Ø·ÙŠÙ‡ Ø°Ø§ÙƒØ±Ø© Ø¬ÙŠØ¯Ø© Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            self.llm = Llama(model_path=self.model_path, n_ctx=4096, verbose=False, n_gpu_layers=0)
            return True
        except Exception as e:
            print(f"âŒ Brain Load Error: {e}")
            return False

    def think(self, user_input, app_context) -> Command:
        if not self.llm:
            return Command(intent="unknown")

        # Ø¯Ù…Ø¬ Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø¹ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        full_prompt = SYSTEM_PROMPT.format(known_apps=app_context, user_input=user_input)

        try:
            output = self.llm(full_prompt, max_tokens=250, temperature=0.1, stop=["<|eot_id|>"])
            text = output['choices'][0]['text'].strip()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON
            if "{" in text:
                json_str = text[text.find('{'):text.rfind('}')+1]
                data = json.loads(json_str)
                # Ø§Ù„ØªØ­Ù‚Ù‚ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ Ø¹Ø¨Ø± Pydantic
                return Command(**data)
            else:
                return Command(intent="unknown")
            
        except Exception as e:
            print(f"âš ï¸ Thinking Error: {e}")
            return Command(intent="unknown")