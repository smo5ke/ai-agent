"""
ğŸ§  LLM Worker Process
=====================
Server Ù…Ø³ØªÙ‚Ù„ ÙŠØ´ØºÙ„ Ø§Ù„Ù€ LLM ÙˆÙŠØªÙˆØ§ØµÙ„ Ø¹Ø¨Ø± TCP Socket.
ÙŠØ¹Ù…Ù„ ÙÙŠ Process Ù…Ù†ÙØµÙ„ Ù„Ø¹Ø²Ù„ Ø§Ù„Ù€ crashes Ø¹Ù† Ø§Ù„Ù€ UI.

Ø§Ù„ØªØ´ØºÙŠÙ„:
    python llm/worker_process.py
"""

import json
import os
import sys
from multiprocessing.connection import Listener

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù€ path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llama_cpp import Llama
from llm.prompts import SYSTEM_PROMPT

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Ù…Ø³Ø§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡)
MODEL_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù€ IPC
ADDRESS = ('localhost', 6000)
AUTHKEY = b"jarvis"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_json(text: str):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON Ù…Ù† Ù†Øµ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ - ÙŠØ¯Ø¹Ù… ÙƒØ§Ø¦Ù† Ø£Ùˆ Ù‚Ø§Ø¦Ù…Ø©"""
    text = text.strip()
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ù‚Ø§Ø¦Ù…Ø© Ø£ÙˆØ§Ù…Ø±
    try:
        start = text.find("[")
        end = text.rfind("]") + 1
        if start != -1 and end > start:
            return json.loads(text[start:end])
    except json.JSONDecodeError:
        pass
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© 2: ÙƒØ§Ø¦Ù† ÙˆØ§Ø­Ø¯
    try:
        start = text.find("{")
        end = text.rfind("}") + 1
        if start != -1 and end > start:
            return json.loads(text[start:end])
    except json.JSONDecodeError:
        pass
    
    return None


def handle_request(llm: Llama, request: dict) -> dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨ ÙˆØ§Ø­Ø¯"""
    try:
        prompt = request.get("prompt", "")
        app_context = request.get("app_context", "")
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt Ø§Ù„ÙƒØ§Ù…Ù„
        full_prompt = SYSTEM_PROMPT.format(
            known_apps=app_context,
            user_input=prompt
        )
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        output = llm(
            full_prompt,
            max_tokens=250,
            temperature=0.1,
            stop=["<|eot_id|>"]
        )
        
        text = output['choices'][0]['text'].strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON
        parsed = extract_json(text)
        
        if parsed:
            return {
                "success": True,
                "response": parsed,
                "raw_text": text
            }
        else:
            return {
                "success": False,
                "error": "No valid JSON in response",
                "raw_text": text
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ù€ Main Server Loop
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def start_worker():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ LLM Worker Server"""
    print("=" * 50)
    print("ğŸ§  LLM Worker Starting...")
    print(f"ğŸ“ Address: {ADDRESS[0]}:{ADDRESS[1]}")
    print(f"ğŸ“¦ Model: {os.path.basename(MODEL_PATH)}")
    print("=" * 50)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ Model not found: {MODEL_PATH}")
        return
    
    print("â³ Loading model (this may take a while)...")
    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=4096,
        n_gpu_layers=0,  # CPU only Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
        verbose=False
    )
    print("âœ… Model loaded successfully!")
    
    # Ø¨Ø¯Ø¡ Ø§Ù„Ù€ Server
    print(f"\nğŸš€ Server listening on {ADDRESS[0]}:{ADDRESS[1]}")
    print("ğŸ“¡ Waiting for connections...\n")
    
    try:
        with Listener(ADDRESS, authkey=AUTHKEY) as listener:
            while True:
                try:
                    conn = listener.accept()
                    print(f"ğŸ“¨ Connection from: {listener.last_accepted}")
                    
                    # Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨
                    request = conn.recv()
                    print(f"ğŸ“ Request: {request.get('prompt', '')[:50]}...")
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨
                    result = handle_request(llm, request)
                    
                    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                    conn.send(result)
                    
                    if result.get("success"):
                        print(f"âœ… Response sent: {result.get('response', {}).get('intent', 'N/A')}")
                    else:
                        print(f"âš ï¸ Error: {result.get('error', 'Unknown')}")
                    
                    conn.close()
                    
                except Exception as e:
                    print(f"âŒ Connection error: {e}")
                    
    except KeyboardInterrupt:
        print("\nğŸ›‘ Worker shutting down...")
    except Exception as e:
        print(f"âŒ Server error: {e}")


if __name__ == "__main__":
    start_worker()
